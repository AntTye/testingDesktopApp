# Project Summary

FastAPI -> PyWebView -> PyInstaller -> .exe file to run local LLM - Apriel, Llama3:8b.

Local LLM context window specialization and specification.

LLM knows what the user wants as context.

To run project you must have ollama installed and any of its local llm installed on your device with enough vram. If not then you will not be able to prompt llm for any answers.

# Minimal Viable Product

1. [x] Integration of any quantized model. 
2. [x] Model output display.
3. [x] User notes storage and user context utilization.
4. [x] App services integrate with one another on possible fronts.
5. [x] Decent user interface and experience.
6. [x] User inputs results in LLM outputs.
7. [x] Start/Executable in file format.
8. [x] No on-line requirements.
9. [x] Tyler Wang must like using it.

# Start Date

- 11/25/2025 : Idea of a local LLM usage.
- 11/27/2025 : White board planning.
- 11/29/2025 : desktop-origin.exe compiled.

![alt text](/README_IMAGES/image1.png)

- 11/30/2025 : Base layout complete.

![alt text](/README_IMAGES/image2.png)

- 12/1/2025: Working Prototype. LLM response can be improved. I see hallucinations.

![alt text](/README_IMAGES/image4.png)
![alt text](/README_IMAGES/image3.png)

- 12/2/2025: Decent UI and UX in my personal opinion. Project fulfills all 9 of my MVP requirements.

# Finished Project 12/2/2025
![alt text](/README_IMAGES/image5.png)
![alt text](/README_IMAGES/image6.png)
